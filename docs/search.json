[
  {
    "objectID": "refresher/001_how_count_vectorizer_works.html",
    "href": "refresher/001_how_count_vectorizer_works.html",
    "title": "python applied machine learning",
    "section": "",
    "text": "CountVectorizer converts a collection of text documents to a matrix of token counts.\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvec = CountVectorizer()\nvec.fit_transform([\"hello world\", \"I have a dog\"]).toarray()\nOutput:\narray([[0, 0, 1, 1],\n       [1, 1, 0, 0]])\nIf the word does not exists, the matrix returned will be all zero.\nprint(vec.transform([\"hello\"]).toarray())\nprint(vec.transform([\"world\"]).toarray())\nprint(vec.transform([\"I have a dog\"]).toarray())\nprint(vec.transform([\"A random sentence\"]).toarray()) # This will return all zeros.\nOutput:\n[[0 0 1 0]]\n[[0 0 0 1]]\n[[1 1 0 0]]\n[[0 0 0 0]]\nNote that the even though our sentences consists of 6 tokens (hello, world, I, have, a, dog), not all words are used as token because single characters are skipped (not because stopwords are filtered).\nTo know which words are part of the vocab, the vocabulary_ method provides the index by token.\nvec.vocabulary_\nOutput:\n{'hello': 2, 'world': 3, 'have': 1, 'dog': 0}",
    "crumbs": [
      "Refresher",
      "CountVectorizer"
    ]
  },
  {
    "objectID": "refresher/001_how_count_vectorizer_works.html#countvectorizer",
    "href": "refresher/001_how_count_vectorizer_works.html#countvectorizer",
    "title": "python applied machine learning",
    "section": "",
    "text": "CountVectorizer converts a collection of text documents to a matrix of token counts.\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvec = CountVectorizer()\nvec.fit_transform([\"hello world\", \"I have a dog\"]).toarray()\nOutput:\narray([[0, 0, 1, 1],\n       [1, 1, 0, 0]])\nIf the word does not exists, the matrix returned will be all zero.\nprint(vec.transform([\"hello\"]).toarray())\nprint(vec.transform([\"world\"]).toarray())\nprint(vec.transform([\"I have a dog\"]).toarray())\nprint(vec.transform([\"A random sentence\"]).toarray()) # This will return all zeros.\nOutput:\n[[0 0 1 0]]\n[[0 0 0 1]]\n[[1 1 0 0]]\n[[0 0 0 0]]\nNote that the even though our sentences consists of 6 tokens (hello, world, I, have, a, dog), not all words are used as token because single characters are skipped (not because stopwords are filtered).\nTo know which words are part of the vocab, the vocabulary_ method provides the index by token.\nvec.vocabulary_\nOutput:\n{'hello': 2, 'world': 3, 'have': 1, 'dog': 0}",
    "crumbs": [
      "Refresher",
      "CountVectorizer"
    ]
  },
  {
    "objectID": "linear_model.html",
    "href": "linear_model.html",
    "title": "python applied machine learning",
    "section": "",
    "text": "import eli5\nimport pandas as pd\nimport sklearn\n\n\ndf = pd.read_csv(\"iris.csv\")\ndf\n\n\n\n\n\n\n\n\nsepal.length\nsepal.width\npetal.length\npetal.width\nvariety\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nSetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nSetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nSetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nSetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nSetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nVirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nVirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nVirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nVirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nVirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\ndf[\"target\"] = LabelEncoder().fit_transform(df.variety)\ndf.head()\n\n\n\n\n\n\n\n\nsepal.length\nsepal.width\npetal.length\npetal.width\nvariety\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nSetosa\n0\n\n\n1\n4.9\n3.0\n1.4\n0.2\nSetosa\n0\n\n\n2\n4.7\n3.2\n1.3\n0.2\nSetosa\n0\n\n\n3\n4.6\n3.1\n1.5\n0.2\nSetosa\n0\n\n\n4\n5.0\n3.6\n1.4\n0.2\nSetosa\n0\n\n\n\n\n\n\n\n\nX = df.drop([\"variety\", \"target\"], axis=1)\ny = df.target\n\nmodel = sklearn.linear_model.LogisticRegression(max_iter=1000)\nmodel.fit(X, y)\n\nLogisticRegression(max_iter=1000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=1000)\n\n\n\neli5.show_weights(model)\n\n\n    \n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n        \n\n    \n        \n\n\n\n\n\n\n\n\ny=0 top features\ny=1 top features\ny=2 top features\n\n\n\n\n\nWeight?\nFeature\n\n\n\n\n+9.850\n&lt;BIAS&gt;\n\n\n+0.967\nx1\n\n\n-0.424\nx0\n\n\n-1.079\nx3\n\n\n-2.517\nx2\n\n\n\n\n\n\nWeight?\nFeature\n\n\n\n\n+2.237\n&lt;BIAS&gt;\n\n\n+0.534\nx0\n\n\n-0.206\nx2\n\n\n-0.322\nx1\n\n\n-0.944\nx3\n\n\n\n\n\n\nWeight?\nFeature\n\n\n\n\n+2.724\nx2\n\n\n+2.024\nx3\n\n\n-0.111\nx0\n\n\n-0.646\nx1\n\n\n-12.088\n&lt;BIAS&gt;\n\n\n\n\n\n\n\n    \n\n    \n        \n\n\n    \n        \n\n\n    \n        \n\n\n    \n\n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n\n\n\n\neli5.explain_weights(model)\n\n\n    \n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n        \n\n    \n        \n\n\n\n\n\n\n\n\ny=0 top features\ny=1 top features\ny=2 top features\n\n\n\n\n\nWeight?\nFeature\n\n\n\n\n+9.850\n&lt;BIAS&gt;\n\n\n+0.967\nx1\n\n\n-0.424\nx0\n\n\n-1.079\nx3\n\n\n-2.517\nx2\n\n\n\n\n\n\nWeight?\nFeature\n\n\n\n\n+2.237\n&lt;BIAS&gt;\n\n\n+0.534\nx0\n\n\n-0.206\nx2\n\n\n-0.322\nx1\n\n\n-0.944\nx3\n\n\n\n\n\n\nWeight?\nFeature\n\n\n\n\n+2.724\nx2\n\n\n+2.024\nx3\n\n\n-0.111\nx0\n\n\n-0.646\nx1\n\n\n-12.088\n&lt;BIAS&gt;\n\n\n\n\n\n\n\n    \n\n    \n        \n\n\n    \n        \n\n\n    \n        \n\n\n    \n\n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n\n\n\n\neli5.explain_prediction(model, X.loc[0])\n\n\n    \n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n        \n\n    \n        \n\n\n\n\n\n\n\n\ny=0 (probability 0.982, score 7.336) top features\ny=1 (probability 0.018, score 3.360) top features\ny=2 (probability 0.000, score -10.695) top features\n\n\n\n\n\nContribution?\nFeature\n\n\n\n\n+9.850\n&lt;BIAS&gt;\n\n\n+3.385\nsepal.width\n\n\n-0.216\npetal.width\n\n\n-2.160\nsepal.length\n\n\n-3.524\npetal.length\n\n\n\n\n\n\nContribution?\nFeature\n\n\n\n\n+2.726\nsepal.length\n\n\n+2.237\n&lt;BIAS&gt;\n\n\n-0.189\npetal.width\n\n\n-0.289\npetal.length\n\n\n-1.125\nsepal.width\n\n\n\n\n\n\nContribution?\nFeature\n\n\n\n\n+3.813\npetal.length\n\n\n+0.405\npetal.width\n\n\n-0.565\nsepal.length\n\n\n-2.260\nsepal.width\n\n\n-12.088\n&lt;BIAS&gt;\n\n\n\n\n\n\n\n    \n\n    \n        \n\n\n    \n        \n\n\n    \n        \n\n\n    \n\n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n\n\n\n\ndf.loc[0]\n\nsepal.length       5.1\nsepal.width        3.5\npetal.length       1.4\npetal.width        0.2\nvariety         Setosa\ntarget               0\nName: 0, dtype: object\n\n\n\ndf.loc[50]\n\nsepal.length           7.0\nsepal.width            3.2\npetal.length           4.7\npetal.width            1.4\nvariety         Versicolor\ntarget                   1\nName: 50, dtype: object\n\n\n\neli5.explain_prediction(model, X.loc[50])\n\n\n    \n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n        \n\n    \n        \n\n\n\n\n\n\n\n\ny=0 (probability 0.002, score -3.361) top features\ny=1 (probability 0.874, score 2.657) top features\ny=2 (probability 0.124, score 0.704) top features\n\n\n\n\n\nContribution?\nFeature\n\n\n\n\n+9.850\n&lt;BIAS&gt;\n\n\n+3.095\nsepal.width\n\n\n-1.511\npetal.width\n\n\n-2.965\nsepal.length\n\n\n-11.831\npetal.length\n\n\n\n\n\n\nContribution?\nFeature\n\n\n\n\n+3.741\nsepal.length\n\n\n+2.237\n&lt;BIAS&gt;\n\n\n-0.970\npetal.length\n\n\n-1.029\nsepal.width\n\n\n-1.322\npetal.width\n\n\n\n\n\n\nContribution?\nFeature\n\n\n\n\n+12.801\npetal.length\n\n\n+2.833\npetal.width\n\n\n-0.776\nsepal.length\n\n\n-2.066\nsepal.width\n\n\n-12.088\n&lt;BIAS&gt;\n\n\n\n\n\n\n\n    \n\n    \n        \n\n\n    \n        \n\n\n    \n        \n\n\n    \n\n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n\n\n\n\ndf.loc[100]\n\nsepal.length          6.3\nsepal.width           3.3\npetal.length          6.0\npetal.width           2.5\nvariety         Virginica\ntarget                  2\nName: 100, dtype: object\n\n\n\neli5.explain_prediction(model, X.loc[100])\n\n\n    \n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n        \n\n    \n        \n\n\n\n\n\n\n\n\ny=0 (probability 0.000, score -7.428) top features\ny=1 (probability 0.004, score 0.944) top features\ny=2 (probability 0.996, score 6.484) top features\n\n\n\n\n\nContribution?\nFeature\n\n\n\n\n+9.850\n&lt;BIAS&gt;\n\n\n+3.192\nsepal.width\n\n\n-2.669\nsepal.length\n\n\n-2.699\npetal.width\n\n\n-15.103\npetal.length\n\n\n\n\n\n\nContribution?\nFeature\n\n\n\n\n+3.367\nsepal.length\n\n\n+2.237\n&lt;BIAS&gt;\n\n\n-1.061\nsepal.width\n\n\n-1.239\npetal.length\n\n\n-2.361\npetal.width\n\n\n\n\n\n\nContribution?\nFeature\n\n\n\n\n+16.342\npetal.length\n\n\n+5.059\npetal.width\n\n\n-0.698\nsepal.length\n\n\n-2.131\nsepal.width\n\n\n-12.088\n&lt;BIAS&gt;",
    "crumbs": [
      "Linear Model"
    ]
  },
  {
    "objectID": "01_debugging_scikit_learn_text_classification_pipeline.html",
    "href": "01_debugging_scikit_learn_text_classification_pipeline.html",
    "title": "python applied machine learning",
    "section": "",
    "text": "from sklearn.datasets import fetch_20newsgroups\n\n# Uncomment this to learn more about the API.\n# help(fetch_20newsgroups)\nWe load the 20 Newsgroups data, keeping only 4 categories.\ncategories = [\"alt.atheism\", \"soc.religion.christian\", \"comp.graphics\", \"sci.med\"]\n\n# We use a fixed random state to ensure the shuffle is deterministic.\ntwenty_train = fetch_20newsgroups(\n    subset=\"train\",\n    categories=categories,\n    shuffle=True,\n    random_state=42,\n)\ntwenty_test = fetch_20newsgroups(\n    subset=\"test\",\n    categories=categories,\n    shuffle=True,\n    random_state=42,\n)\nWe create a basic text processing pipeline - bag of words features and Logistic Regression as a classifier.\nSee How Count Vectorizer works\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.pipeline import make_pipeline\n\nvec = CountVectorizer()\n\n# We increase the number of iterations to prevent the error below:\n# ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT\nclf = LogisticRegressionCV(max_iter=1_000)\npipe = make_pipeline(vec, clf)\npipe.fit(twenty_train.data, twenty_train.target)\n\nPipeline(steps=[('countvectorizer', CountVectorizer()),\n                ('logisticregressioncv', LogisticRegressionCV(max_iter=1000))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('countvectorizer', CountVectorizer()),\n                ('logisticregressioncv', LogisticRegressionCV(max_iter=1000))])CountVectorizerCountVectorizer()LogisticRegressionCVLogisticRegressionCV(max_iter=1000)\nTo check the quality of the model, we run it against the test data.\nfrom sklearn import metrics\n\n\ndef print_report(pipe):\n    y_test = twenty_test.target\n    y_pred = pipe.predict(twenty_test.data)\n    report = metrics.classification_report(\n        y_test, y_pred, target_names=twenty_test.target_names\n    )\n    print(report)\n    print(\"accuracy: {:0.3f}\".format(metrics.accuracy_score(y_test, y_pred)))\nprint_report(pipe)\n\n                        precision    recall  f1-score   support\n\n           alt.atheism       0.91      0.81      0.86       319\n         comp.graphics       0.86      0.94      0.90       389\n               sci.med       0.92      0.81      0.86       396\nsoc.religion.christian       0.88      0.98      0.93       398\n\n              accuracy                           0.89      1502\n             macro avg       0.89      0.89      0.89      1502\n          weighted avg       0.89      0.89      0.89      1502\n\naccuracy: 0.889\nThe model achieve a relatively high accuracy. To understand what the model learned, we use the eli5.show_weights function. The function accepts the classifier as the first argument:\nimport eli5\n\neli5.show_weights(clf, vec=vec, top=10, target_names=twenty_test.target_names)\n\n\n    \n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n        \n\n    \n        \n\n\n\n\n\n\n\n\n\ny=alt.atheism top features\ny=comp.graphics top features\ny=sci.med top features\ny=soc.religion.christian top features\n\n\n\n\n\nWeight?\nFeature\n\n\n\n\n+0.569\nx19218\n\n\n+0.482\nx5714\n\n\n+0.474\nx21167\n\n\n+0.473\nx6597\n\n\n+0.429\nx5718\n\n\n+0.416\nx23677\n\n\n+0.394\nx35350\n\n\n+0.377\nx6472\n\n\n… 11111 more positive …\n\n\n… 24668 more negative …\n\n\n-0.394\nx28473\n\n\n-2.277\n&lt;BIAS&gt;\n\n\n\n\n\n\nWeight?\nFeature\n\n\n\n\n+1.084\nx15699\n\n\n+0.870\n&lt;BIAS&gt;\n\n\n+0.508\nx17366\n\n\n+0.496\nx30117\n\n\n+0.495\nx14277\n\n\n+0.488\nx17356\n\n\n+0.487\nx14281\n\n\n+0.454\nx24267\n\n\n+0.450\nx7874\n\n\n… 11561 more positive …\n\n\n… 24218 more negative …\n\n\n-0.438\nx24784\n\n\n\n\n\n\nWeight?\nFeature\n\n\n\n\n+0.590\nx17854\n\n\n+0.580\nx25234\n\n\n+0.580\nx12026\n\n\n+0.486\nx11729\n\n\n+0.477\nx22379\n\n\n+0.472\nx32847\n\n\n+0.417\nx16328\n\n\n+0.409\nx26801\n\n\n… 13945 more positive …\n\n\n… 21834 more negative …\n\n\n-0.518\nx15699\n\n\n-0.538\nx15521\n\n\n\n\n\n\nWeight?\nFeature\n\n\n\n\n+1.417\n&lt;BIAS&gt;\n\n\n+0.704\nx28473\n\n\n+0.643\nx8609\n\n\n+0.632\nx8559\n\n\n+0.584\nx8553\n\n\n+0.542\nx15521\n\n\n+0.510\nx8544\n\n\n… 12505 more positive …\n\n\n… 23274 more negative …\n\n\n-0.565\nx25663\n\n\n-0.765\nx23122\n\n\n-0.796\nx16881\nHow to interpret:\nDocuments with words graphics, images are closely related to the class comp.graphics.\neli5.show_prediction(\n    clf, twenty_test.data[0], vec=vec, target_names=twenty_test.target_names\n)\n\n\n    \n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n        \n\n    \n\n    \n        \n    \n        \n        \n    \n        \n            \n    \n        y=alt.atheism\n    \n\n\n    \n    (probability 0.037, score -0.119)\n\ntop features\n        \n    \n    \n\n\n\nContribution?\nFeature\n\n\n\n\n+0.816\nx7207\n\n\n+0.518\nx12626\n\n\n+0.295\nx431\n\n\n+0.240\nx20253\n\n\n+0.218\nx25663\n\n\n+0.211\nx27031\n\n\n+0.211\nx32142\n\n\n+0.205\nx16881\n\n\n+0.200\nx4720\n\n\n+0.189\nx33773\n\n\n+0.182\nx23122\n\n\n+0.165\nx6298\n\n\n+0.161\nx23870\n\n\n+0.124\nx14887\n\n\n+0.118\nx18510\n\n\n+0.117\nx23301\n\n\n+0.117\nx32202\n\n\n+0.089\nx12014\n\n\n+0.072\nx29455\n\n\n+0.070\nx34703\n\n\n+0.067\nx21506\n\n\n+0.046\nx24108\n\n\n+0.041\nx17556\n\n\n+0.038\nx12711\n\n\n+0.037\nx7860\n\n\n+0.029\nx34755\n\n\n+0.028\nx24532\n\n\n+0.027\nx24089\n\n\n+0.019\nx23610\n\n\n+0.018\nx5203\n\n\n+0.016\nx5549\n\n\n+0.016\nx31865\n\n\n+0.012\nx25472\n\n\n+0.010\nx13263\n\n\n+0.009\nx32139\n\n\n+0.008\nx13583\n\n\n+0.000\nx7090\n\n\n-0.000\nx11086\n\n\n-0.000\nx27498\n\n\n-0.000\nx31413\n\n\n-0.002\nx8453\n\n\n-0.005\nx23733\n\n\n-0.005\nx16772\n\n\n-0.008\nx4992\n\n\n-0.008\nx13813\n\n\n-0.009\nx23915\n\n\n-0.012\nx16133\n\n\n-0.013\nx22750\n\n\n-0.013\nx20585\n\n\n-0.015\nx15601\n\n\n-0.016\nx8457\n\n\n-0.019\nx31077\n\n\n-0.020\nx35260\n\n\n-0.024\nx17050\n\n\n-0.024\nx7265\n\n\n-0.028\nx20041\n\n\n-0.028\nx26986\n\n\n-0.029\nx22541\n\n\n-0.029\nx32233\n\n\n-0.034\nx21389\n\n\n-0.044\nx32160\n\n\n-0.046\nx30245\n\n\n-0.056\nx27135\n\n\n-0.058\nx14601\n\n\n-0.064\nx30865\n\n\n-0.067\nx19334\n\n\n-0.075\nx16418\n\n\n-0.075\nx24296\n\n\n-0.078\nx4827\n\n\n-0.090\nx19076\n\n\n-0.091\nx7766\n\n\n-0.091\nx16014\n\n\n-0.115\nx5195\n\n\n-0.156\nx16254\n\n\n-0.161\nx33216\n\n\n-0.211\nx32493\n\n\n-0.233\nx3958\n\n\n-0.247\nx34935\n\n\n-0.284\nx35157\n\n\n-2.277\n&lt;BIAS&gt;\n\n\n\n\n\n    \n\n\n\n    \n        from: brian@ucsd.edu (brian kantor)\nsubject: re: help for kidney stones ..............\norganization: the avant-garde of the now, ltd.\nlines: 12\nnntp-posting-host: ucsd.edu\n\nas i recall from my bout with kidney stones, there isn't any\nmedication that can do anything about them except relieve the pain.\n\neither they pass, or they have to be broken up with sound, or they have\nto be extracted surgically.\n\nwhen i was in, the x-ray tech happened to mention that she'd had kidney\nstones and children, and the childbirth hurt less.\n\ndemerol worked, although i nearly got arrested on my way home when i barfed\nall over the police car parked just outside the er.\n    - brian\n\n    \n\n    \n        \n    \n        \n        \n    \n        \n            \n    \n        y=comp.graphics\n    \n\n\n    \n    (probability 0.020, score -0.740)\n\ntop features\n        \n    \n    \n\n\n\nContribution?\nFeature\n\n\n\n\n+0.870\n&lt;BIAS&gt;\n\n\n+0.346\nx16254\n\n\n+0.336\nx16881\n\n\n+0.324\nx23122\n\n\n+0.252\nx23870\n\n\n+0.227\nx35157\n\n\n+0.218\nx14601\n\n\n+0.209\nx25663\n\n\n+0.178\nx3958\n\n\n+0.173\nx23733\n\n\n+0.166\nx16418\n\n\n+0.162\nx26986\n\n\n+0.157\nx14887\n\n\n+0.149\nx7766\n\n\n+0.127\nx15601\n\n\n+0.118\nx23915\n\n\n+0.114\nx32202\n\n\n+0.109\nx5203\n\n\n+0.069\nx20253\n\n\n+0.065\nx4827\n\n\n+0.060\nx24108\n\n\n+0.057\nx16772\n\n\n+0.044\nx32160\n\n\n+0.041\nx22750\n\n\n+0.036\nx12711\n\n\n+0.028\nx20585\n\n\n+0.021\nx31865\n\n\n+0.019\nx4720\n\n\n+0.019\nx5195\n\n\n+0.015\nx19076\n\n\n+0.015\nx13263\n\n\n+0.002\nx25472\n\n\n-0.000\nx8453\n\n\n-0.000\nx7090\n\n\n-0.000\nx11086\n\n\n-0.002\nx27498\n\n\n-0.004\nx7860\n\n\n-0.010\nx27135\n\n\n-0.010\nx34755\n\n\n-0.011\nx13813\n\n\n-0.013\nx16014\n\n\n-0.015\nx21506\n\n\n-0.015\nx31413\n\n\n-0.016\nx24532\n\n\n-0.017\nx17050\n\n\n-0.021\nx13583\n\n\n-0.022\nx7265\n\n\n-0.027\nx33773\n\n\n-0.028\nx23301\n\n\n-0.031\nx24089\n\n\n-0.038\nx12014\n\n\n-0.041\nx35260\n\n\n-0.041\nx18510\n\n\n-0.042\nx16133\n\n\n-0.045\nx32139\n\n\n-0.046\nx21389\n\n\n-0.048\nx8457\n\n\n-0.057\nx30245\n\n\n-0.058\nx31077\n\n\n-0.059\nx20041\n\n\n-0.062\nx32493\n\n\n-0.115\nx17556\n\n\n-0.116\nx34703\n\n\n-0.117\nx34935\n\n\n-0.133\nx24296\n\n\n-0.133\nx23610\n\n\n-0.158\nx5549\n\n\n-0.172\nx27031\n\n\n-0.177\nx12626\n\n\n-0.184\nx431\n\n\n-0.186\nx30865\n\n\n-0.202\nx19334\n\n\n-0.221\nx32142\n\n\n-0.223\nx4992\n\n\n-0.280\nx33216\n\n\n-0.314\nx29455\n\n\n-0.326\nx6298\n\n\n-0.402\nx32233\n\n\n-0.589\nx7207\n\n\n-0.644\nx22541\n\n\n\n\n\n    \n\n\n\n    \n        from: brian@ucsd.edu (brian kantor)\nsubject: re: help for kidney stones ..............\norganization: the avant-garde of the now, ltd.\nlines: 12\nnntp-posting-host: ucsd.edu\n\nas i recall from my bout with kidney stones, there isn't any\nmedication that can do anything about them except relieve the pain.\n\neither they pass, or they have to be broken up with sound, or they have\nto be extracted surgically.\n\nwhen i was in, the x-ray tech happened to mention that she'd had kidney\nstones and children, and the childbirth hurt less.\n\ndemerol worked, although i nearly got arrested on my way home when i barfed\nall over the police car parked just outside the er.\n    - brian\n\n    \n\n    \n        \n    \n        \n        \n    \n        \n            \n    \n        y=sci.med\n    \n\n\n    \n    (probability 0.938, score 3.106)\n\ntop features\n        \n    \n    \n\n\n\nContribution?\nFeature\n\n\n\n\n+0.522\nx33216\n\n\n+0.513\nx22541\n\n\n+0.381\nx19334\n\n\n+0.293\nx30865\n\n\n+0.265\nx34935\n\n\n+0.260\nx23122\n\n\n+0.255\nx16881\n\n\n+0.251\nx29455\n\n\n+0.213\nx24296\n\n\n+0.209\nx4992\n\n\n+0.199\nx16418\n\n\n+0.185\nx16014\n\n\n+0.163\nx21389\n\n\n+0.153\nx7766\n\n\n+0.153\nx23915\n\n\n+0.137\nx25663\n\n\n+0.131\nx7207\n\n\n+0.128\nx27135\n\n\n+0.118\nx30245\n\n\n+0.106\nx6298\n\n\n+0.105\nx20041\n\n\n+0.094\nx32493\n\n\n+0.086\nx5195\n\n\n+0.083\nx3958\n\n\n+0.079\nx27031\n\n\n+0.073\nx35260\n\n\n+0.070\nx23610\n\n\n+0.066\nx12626\n\n\n+0.052\nx16133\n\n\n+0.048\nx20585\n\n\n+0.046\nx31865\n\n\n+0.045\nx5549\n\n\n+0.025\nx24089\n\n\n+0.018\nx31413\n\n\n+0.016\nx13813\n\n\n+0.015\nx17050\n\n\n+0.015\nx7265\n\n\n+0.013\nx17556\n\n\n+0.010\nx35157\n\n\n+0.006\nx19076\n\n\n+0.003\nx27498\n\n\n+0.002\nx8453\n\n\n+0.000\nx11086\n\n\n-0.000\nx7090\n\n\n-0.002\nx25472\n\n\n-0.005\nx13583\n\n\n-0.005\nx7860\n\n\n-0.007\nx33773\n\n\n-0.009\nx26986\n\n\n-0.010\nx32233\n\n\n-0.010\n&lt;BIAS&gt;\n\n\n-0.013\nx4827\n\n\n-0.014\nx23733\n\n\n-0.014\nx13263\n\n\n-0.019\nx16772\n\n\n-0.019\nx431\n\n\n-0.020\nx22750\n\n\n-0.021\nx34703\n\n\n-0.023\nx14601\n\n\n-0.025\nx21506\n\n\n-0.027\nx32160\n\n\n-0.028\nx8457\n\n\n-0.037\nx31077\n\n\n-0.038\nx24532\n\n\n-0.051\nx34755\n\n\n-0.053\nx4720\n\n\n-0.062\nx12711\n\n\n-0.068\nx24108\n\n\n-0.071\nx32202\n\n\n-0.074\nx32139\n\n\n-0.077\nx14887\n\n\n-0.118\nx18510\n\n\n-0.127\nx16254\n\n\n-0.129\nx23301\n\n\n-0.147\nx15601\n\n\n-0.168\nx5203\n\n\n-0.176\nx20253\n\n\n-0.208\nx12014\n\n\n-0.211\nx23870\n\n\n-0.414\nx32142\n\n\n\n\n\n    \n\n\n\n    \n        from: brian@ucsd.edu (brian kantor)\nsubject: re: help for kidney stones ..............\norganization: the avant-garde of the now, ltd.\nlines: 12\nnntp-posting-host: ucsd.edu\n\nas i recall from my bout with kidney stones, there isn't any\nmedication that can do anything about them except relieve the pain.\n\neither they pass, or they have to be broken up with sound, or they have\nto be extracted surgically.\n\nwhen i was in, the x-ray tech happened to mention that she'd had kidney\nstones and children, and the childbirth hurt less.\n\ndemerol worked, although i nearly got arrested on my way home when i barfed\nall over the police car parked just outside the er.\n    - brian\n\n    \n\n    \n        \n    \n        \n        \n    \n        \n            \n    \n        y=soc.religion.christian\n    \n\n\n    \n    (probability 0.004, score -2.246)\n\ntop features\n        \n    \n    \n\n\n\nContribution?\nFeature\n\n\n\n\n+1.417\n&lt;BIAS&gt;\n\n\n+0.441\nx32233\n\n\n+0.424\nx32142\n\n\n+0.178\nx32493\n\n\n+0.159\nx22541\n\n\n+0.156\nx12014\n\n\n+0.114\nx31077\n\n\n+0.110\nx32139\n\n\n+0.099\nx34935\n\n\n+0.097\nx5549\n\n\n+0.092\nx8457\n\n\n+0.068\nx19076\n\n\n+0.066\nx34703\n\n\n+0.060\nx17556\n\n\n+0.055\nx6298\n\n\n+0.047\nx35157\n\n\n+0.044\nx23610\n\n\n+0.041\nx18510\n\n\n+0.041\nx5203\n\n\n+0.040\nx23301\n\n\n+0.035\nx15601\n\n\n+0.032\nx34755\n\n\n+0.031\nx7265\n\n\n+0.027\nx32160\n\n\n+0.026\nx17050\n\n\n+0.026\nx24532\n\n\n+0.025\nx4827\n\n\n+0.022\nx4992\n\n\n+0.018\nx13583\n\n\n+0.010\nx5195\n\n\n+0.003\nx13813\n\n\n+0.002\nx16133\n\n\n-0.000\nx11086\n\n\n-0.000\nx7090\n\n\n-0.000\nx8453\n\n\n-0.001\nx27498\n\n\n-0.002\nx31413\n\n\n-0.006\nx24296\n\n\n-0.008\nx22750\n\n\n-0.008\nx29455\n\n\n-0.010\nx13263\n\n\n-0.012\nx35260\n\n\n-0.012\nx25472\n\n\n-0.012\nx12711\n\n\n-0.015\nx30245\n\n\n-0.017\nx20041\n\n\n-0.021\nx24089\n\n\n-0.028\nx7860\n\n\n-0.028\nx21506\n\n\n-0.028\nx3958\n\n\n-0.033\nx16772\n\n\n-0.038\nx24108\n\n\n-0.043\nx30865\n\n\n-0.062\nx20585\n\n\n-0.062\nx27135\n\n\n-0.063\nx16254\n\n\n-0.080\nx16014\n\n\n-0.081\nx33216\n\n\n-0.082\nx21389\n\n\n-0.084\nx31865\n\n\n-0.092\nx431\n\n\n-0.112\nx19334\n\n\n-0.118\nx27031\n\n\n-0.125\nx26986\n\n\n-0.133\nx20253\n\n\n-0.138\nx14601\n\n\n-0.154\nx23733\n\n\n-0.155\nx33773\n\n\n-0.159\nx32202\n\n\n-0.166\nx4720\n\n\n-0.202\nx23870\n\n\n-0.204\nx14887\n\n\n-0.212\nx7766\n\n\n-0.262\nx23915\n\n\n-0.290\nx16418\n\n\n-0.357\nx7207\n\n\n-0.407\nx12626\n\n\n-0.565\nx25663\n\n\n-0.765\nx23122\n\n\n-0.796\nx16881\n\n\n\n\n\n    \n\n\n\n    \n        from: brian@ucsd.edu (brian kantor)\nsubject: re: help for kidney stones ..............\norganization: the avant-garde of the now, ltd.\nlines: 12\nnntp-posting-host: ucsd.edu\n\nas i recall from my bout with kidney stones, there isn't any\nmedication that can do anything about them except relieve the pain.\n\neither they pass, or they have to be broken up with sound, or they have\nto be extracted surgically.\n\nwhen i was in, the x-ray tech happened to mention that she'd had kidney\nstones and children, and the childbirth hurt less.\n\ndemerol worked, although i nearly got arrested on my way home when i barfed\nall over the police car parked just outside the er.\n    - brian\nWe can visualize how the predictions are made, and the highlighted feature (words) that contributes to the prediction.\nWe notice that the classifier learned some non-interesting data, such as email addresses. To improve the model performance, we should perform cleaning.\nWe can take that step by first removing the headers and footers.\ntwenty_train = fetch_20newsgroups(\n    subset=\"train\",\n    categories=categories,\n    shuffle=True,\n    random_state=42,\n    remove=[\"headers\", \"footers\"],\n)\n\ntwenty_test = fetch_20newsgroups(\n    subset=\"test\",\n    categories=categories,\n    shuffle=True,\n    random_state=42,\n    remove=[\"headers\", \"footers\"],\n)\n\nvec = CountVectorizer()\nclf = LogisticRegressionCV(max_iter=1_000)\npipe = make_pipeline(vec, clf)\npipe.fit(twenty_train.data, twenty_train.target);\nprint_report(pipe)\n\n                        precision    recall  f1-score   support\n\n           alt.atheism       0.81      0.76      0.79       319\n         comp.graphics       0.82      0.93      0.87       389\n               sci.med       0.87      0.78      0.82       396\nsoc.religion.christian       0.86      0.88      0.87       398\n\n              accuracy                           0.84      1502\n             macro avg       0.84      0.84      0.84      1502\n          weighted avg       0.84      0.84      0.84      1502\n\naccuracy: 0.840\nThe accuracy of the model somehow becomes worse (from 0.889 to 0.840).\neli5.show_prediction(\n    clf, twenty_test.data[0], vec=vec, target_names=twenty_test.target_names\n)\n\n\n    \n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n        \n\n    \n\n    \n        \n    \n        \n        \n    \n        \n            \n    \n        y=alt.atheism\n    \n\n\n    \n    (probability 0.028, score -1.583)\n\ntop features\n        \n    \n    \n\n\n\nContribution?\nFeature\n\n\n\n\n+0.206\nx30691\n\n\n+0.192\nx16763\n\n\n+0.164\nx10768\n\n\n+0.102\nx12213\n\n\n+0.093\nx19407\n\n\n+0.080\nx26731\n\n\n+0.068\nx29186\n\n\n+0.067\nx29205\n\n\n+0.061\nx21567\n\n\n+0.060\nx31529\n\n\n+0.056\nx4484\n\n\n+0.054\nx29189\n\n\n+0.048\nx14675\n\n\n+0.041\nx5499\n\n\n+0.040\nx4806\n\n\n+0.038\nx14563\n\n\n+0.036\nx11390\n\n\n+0.019\nx29247\n\n\n+0.012\nx22180\n\n\n+0.006\nx24471\n\n\n+0.005\nx6218\n\n\n+0.005\nx6373\n\n\n-0.000\nx24953\n\n\n-0.000\nx28553\n\n\n-0.002\nx7428\n\n\n-0.003\nx13444\n\n\n-0.006\nx12438\n\n\n-0.011\nx7432\n\n\n-0.015\nx4286\n\n\n-0.021\nx17471\n\n\n-0.026\nx19303\n\n\n-0.027\nx28037\n\n\n-0.028\nx24609\n\n\n-0.034\nx18088\n\n\n-0.035\nx15393\n\n\n-0.036\nx28955\n\n\n-0.063\nx20353\n\n\n-0.063\nx29277\n\n\n-0.072\nx15864\n\n\n-0.077\nx27454\n\n\n-0.082\nx21961\n\n\n-0.093\nx14456\n\n\n-0.165\nx31740\n\n\n-0.174\nx4476\n\n\n-0.184\nx3338\n\n\n-0.216\nx29521\n\n\n-0.250\nx6815\n\n\n-0.351\nx31934\n\n\n-1.002\n&lt;BIAS&gt;\n\n\n\n\n\n    \n\n\n\n    \n        as i recall from my bout with kidney stones, there isn't any\nmedication that can do anything about them except relieve the pain.\n\neither they pass, or they have to be broken up with sound, or they have\nto be extracted surgically.\n\nwhen i was in, the x-ray tech happened to mention that she'd had kidney\nstones and children, and the childbirth hurt less.\n    \n\n    \n        \n    \n        \n        \n    \n        \n            \n    \n        y=comp.graphics\n    \n\n\n    \n    (probability 0.077, score -0.570)\n\ntop features\n        \n    \n    \n\n\n\nContribution?\nFeature\n\n\n\n\n+0.959\n&lt;BIAS&gt;\n\n\n+0.219\nx31934\n\n\n+0.216\nx21567\n\n\n+0.169\nx13444\n\n\n+0.160\nx29247\n\n\n+0.154\nx6815\n\n\n+0.142\nx24471\n\n\n+0.075\nx14675\n\n\n+0.061\nx3338\n\n\n+0.058\nx11390\n\n\n+0.055\nx4476\n\n\n+0.042\nx28955\n\n\n+0.037\nx29205\n\n\n-0.000\nx7428\n\n\n-0.000\nx6218\n\n\n-0.002\nx24953\n\n\n-0.009\nx28553\n\n\n-0.009\nx12438\n\n\n-0.010\nx16763\n\n\n-0.011\nx22180\n\n\n-0.013\nx6373\n\n\n-0.016\nx15393\n\n\n-0.018\nx19407\n\n\n-0.032\nx4484\n\n\n-0.041\nx19303\n\n\n-0.042\nx14456\n\n\n-0.044\nx18088\n\n\n-0.046\nx17471\n\n\n-0.047\nx24609\n\n\n-0.049\nx28037\n\n\n-0.050\nx27454\n\n\n-0.061\nx31529\n\n\n-0.061\nx15864\n\n\n-0.063\nx14563\n\n\n-0.069\nx7432\n\n\n-0.085\nx29189\n\n\n-0.088\nx31740\n\n\n-0.094\nx12213\n\n\n-0.096\nx4286\n\n\n-0.097\nx29521\n\n\n-0.103\nx21961\n\n\n-0.112\nx30691\n\n\n-0.125\nx4806\n\n\n-0.145\nx29186\n\n\n-0.166\nx10768\n\n\n-0.212\nx5499\n\n\n-0.270\nx26731\n\n\n-0.297\nx20353\n\n\n-0.337\nx29277\n\n\n\n\n\n    \n\n\n\n    \n        as i recall from my bout with kidney stones, there isn't any\nmedication that can do anything about them except relieve the pain.\n\neither they pass, or they have to be broken up with sound, or they have\nto be extracted surgically.\n\nwhen i was in, the x-ray tech happened to mention that she'd had kidney\nstones and children, and the childbirth hurt less.\n    \n\n    \n        \n    \n        \n        \n    \n        \n            \n    \n        y=sci.med\n    \n\n\n    \n    (probability 0.643, score 1.547)\n\ntop features\n        \n    \n    \n\n\n\nContribution?\nFeature\n\n\n\n\n+0.255\nx26731\n\n\n+0.226\nx20353\n\n\n+0.217\nx29521\n\n\n+0.202\nx21961\n\n\n+0.193\nx31740\n\n\n+0.185\nx29277\n\n\n+0.181\nx6815\n\n\n+0.178\nx14456\n\n\n+0.166\nx24609\n\n\n+0.166\nx27454\n\n\n+0.151\nx3338\n\n\n+0.129\nx4286\n\n\n+0.128\nx19303\n\n\n+0.112\n&lt;BIAS&gt;\n\n\n+0.104\nx18088\n\n\n+0.101\nx4476\n\n\n+0.093\nx17471\n\n\n+0.065\nx15864\n\n\n+0.021\nx14563\n\n\n+0.019\nx31934\n\n\n+0.018\nx28955\n\n\n+0.016\nx12438\n\n\n+0.016\nx15393\n\n\n+0.015\nx4806\n\n\n+0.010\nx28553\n\n\n+0.004\nx24953\n\n\n+0.002\nx7428\n\n\n-0.003\nx6218\n\n\n-0.004\nx12213\n\n\n-0.009\nx30691\n\n\n-0.018\nx7432\n\n\n-0.022\nx6373\n\n\n-0.025\nx19407\n\n\n-0.031\nx28037\n\n\n-0.035\nx5499\n\n\n-0.042\nx14675\n\n\n-0.044\nx31529\n\n\n-0.045\nx22180\n\n\n-0.046\nx29186\n\n\n-0.051\nx29205\n\n\n-0.056\nx13444\n\n\n-0.056\nx29247\n\n\n-0.078\nx24471\n\n\n-0.083\nx11390\n\n\n-0.089\nx16763\n\n\n-0.120\nx4484\n\n\n-0.131\nx21567\n\n\n-0.220\nx10768\n\n\n-0.221\nx29189\n\n\n\n\n\n    \n\n\n\n    \n        as i recall from my bout with kidney stones, there isn't any\nmedication that can do anything about them except relieve the pain.\n\neither they pass, or they have to be broken up with sound, or they have\nto be extracted surgically.\n\nwhen i was in, the x-ray tech happened to mention that she'd had kidney\nstones and children, and the childbirth hurt less.\n    \n\n    \n        \n    \n        \n        \n    \n        \n            \n    \n        y=soc.religion.christian\n    \n\n\n    \n    (probability 0.251, score 0.606)\n\ntop features\n        \n    \n    \n\n\n\nContribution?\nFeature\n\n\n\n\n+0.251\nx29189\n\n\n+0.221\nx10768\n\n\n+0.216\nx29277\n\n\n+0.206\nx5499\n\n\n+0.134\nx20353\n\n\n+0.123\nx29186\n\n\n+0.113\nx31934\n\n\n+0.107\nx28037\n\n\n+0.099\nx7432\n\n\n+0.096\nx4484\n\n\n+0.095\nx29521\n\n\n+0.071\nx4806\n\n\n+0.069\nx15864\n\n\n+0.060\nx31740\n\n\n+0.045\nx31529\n\n\n+0.044\nx22180\n\n\n+0.035\nx15393\n\n\n+0.029\nx6373\n\n\n+0.017\nx4476\n\n\n+0.003\nx14563\n\n\n-0.000\nx7428\n\n\n-0.001\nx12438\n\n\n-0.001\nx28553\n\n\n-0.002\nx24953\n\n\n-0.003\nx6218\n\n\n-0.004\nx12213\n\n\n-0.011\nx11390\n\n\n-0.017\nx21961\n\n\n-0.018\nx4286\n\n\n-0.024\nx28955\n\n\n-0.026\nx18088\n\n\n-0.026\nx17471\n\n\n-0.027\nx3338\n\n\n-0.039\nx27454\n\n\n-0.043\nx14456\n\n\n-0.050\nx19407\n\n\n-0.052\nx29205\n\n\n-0.062\nx19303\n\n\n-0.065\nx26731\n\n\n-0.069\n&lt;BIAS&gt;\n\n\n-0.070\nx24471\n\n\n-0.081\nx14675\n\n\n-0.085\nx30691\n\n\n-0.086\nx6815\n\n\n-0.092\nx24609\n\n\n-0.094\nx16763\n\n\n-0.110\nx13444\n\n\n-0.124\nx29247\n\n\n-0.146\nx21567\n\n\n\n\n\n    \n\n\n\n    \n        as i recall from my bout with kidney stones, there isn't any\nmedication that can do anything about them except relieve the pain.\n\neither they pass, or they have to be broken up with sound, or they have\nto be extracted surgically.\n\nwhen i was in, the x-ray tech happened to mention that she'd had kidney\nstones and children, and the childbirth hurt less.\nEmail addresses no longer appears in the result, but the classifier still doesn’t look good. The classifier seems to assign high weights to non-related words such as my, to.",
    "crumbs": [
      "Pipeline Improvement"
    ]
  },
  {
    "objectID": "01_debugging_scikit_learn_text_classification_pipeline.html#pipeline-improvement",
    "href": "01_debugging_scikit_learn_text_classification_pipeline.html#pipeline-improvement",
    "title": "python applied machine learning",
    "section": "1 Pipeline Improvement",
    "text": "1 Pipeline Improvement\nTo improve the classifier, we may filter out stop words.\n\nvec = CountVectorizer(stop_words=\"english\")\nclf = LogisticRegressionCV(max_iter=1_000)\npipe = make_pipeline(vec, clf)\npipe.fit(twenty_train.data, twenty_train.target)\n\nprint_report(pipe)\n\n                        precision    recall  f1-score   support\n\n           alt.atheism       0.86      0.76      0.81       319\n         comp.graphics       0.85      0.94      0.89       389\n               sci.med       0.92      0.85      0.88       396\nsoc.religion.christian       0.86      0.89      0.87       398\n\n              accuracy                           0.87      1502\n             macro avg       0.87      0.86      0.86      1502\n          weighted avg       0.87      0.87      0.87      1502\n\naccuracy: 0.868\n\n\n\neli5.show_prediction(\n    clf, twenty_test.data[0], vec=vec, target_names=twenty_test.target_names\n)\n\n\n    \n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n        \n\n    \n\n    \n        \n    \n        \n        \n    \n        \n            \n    \n        y=alt.atheism\n    \n\n\n    \n    (probability 0.064, score -1.038)\n\ntop features\n        \n    \n    \n\n\n\nContribution?\nFeature\n\n\n\n\n+0.209\nx16630\n\n\n+0.092\nx19260\n\n\n+0.049\nx14454\n\n\n+0.025\nx6321\n\n\n+0.008\nx21986\n\n\n+0.007\nx6166\n\n\n-0.000\nx24753\n\n\n-0.001\nx28327\n\n\n-0.004\nx7370\n\n\n-0.010\nx24272\n\n\n-0.012\nx12351\n\n\n-0.018\nx7374\n\n\n-0.025\nx24409\n\n\n-0.036\nx15268\n\n\n-0.039\nx19156\n\n\n-0.041\nx17334\n\n\n-0.042\nx27812\n\n\n-0.045\nx28727\n\n\n-0.098\nx27230\n\n\n-0.102\nx21768\n\n\n-0.954\n&lt;BIAS&gt;\n\n\n\n\n\n    \n\n\n\n    \n        as i recall from my bout with kidney stones, there isn't any\nmedication that can do anything about them except relieve the pain.\n\neither they pass, or they have to be broken up with sound, or they have\nto be extracted surgically.\n\nwhen i was in, the x-ray tech happened to mention that she'd had kidney\nstones and children, and the childbirth hurt less.\n    \n\n    \n        \n    \n        \n        \n    \n        \n            \n    \n        y=comp.graphics\n    \n\n\n    \n    (probability 0.194, score 0.078)\n\ntop features\n        \n    \n    \n\n\n\nContribution?\nFeature\n\n\n\n\n+0.732\n&lt;BIAS&gt;\n\n\n+0.210\nx24272\n\n\n+0.039\nx28727\n\n\n-0.000\nx7370\n\n\n-0.000\nx6166\n\n\n-0.001\nx12351\n\n\n-0.004\nx24753\n\n\n-0.008\nx28327\n\n\n-0.010\nx15268\n\n\n-0.012\nx19260\n\n\n-0.019\nx21986\n\n\n-0.024\nx16630\n\n\n-0.026\nx6321\n\n\n-0.054\nx27230\n\n\n-0.074\nx24409\n\n\n-0.083\nx17334\n\n\n-0.088\nx19156\n\n\n-0.097\nx7374\n\n\n-0.119\nx14454\n\n\n-0.131\nx27812\n\n\n-0.153\nx21768\n\n\n\n\n\n    \n\n\n\n    \n        as i recall from my bout with kidney stones, there isn't any\nmedication that can do anything about them except relieve the pain.\n\neither they pass, or they have to be broken up with sound, or they have\nto be extracted surgically.\n\nwhen i was in, the x-ray tech happened to mention that she'd had kidney\nstones and children, and the childbirth hurt less.\n    \n\n    \n        \n    \n        \n        \n    \n        \n            \n    \n        y=sci.med\n    \n\n\n    \n    (probability 0.603, score 1.212)\n\ntop features\n        \n    \n    \n\n\n\nContribution?\nFeature\n\n\n\n\n+0.298\nx21768\n\n\n+0.212\nx19156\n\n\n+0.210\nx17334\n\n\n+0.206\nx27230\n\n\n+0.176\n&lt;BIAS&gt;\n\n\n+0.176\nx24409\n\n\n+0.092\nx27812\n\n\n+0.055\nx14454\n\n\n+0.034\nx28727\n\n\n+0.020\nx15268\n\n\n+0.014\nx28327\n\n\n+0.006\nx12351\n\n\n+0.004\nx7370\n\n\n+0.004\nx24753\n\n\n-0.000\nx6166\n\n\n-0.015\nx7374\n\n\n-0.023\nx6321\n\n\n-0.026\nx19260\n\n\n-0.041\nx21986\n\n\n-0.093\nx16630\n\n\n-0.096\nx24272\n\n\n\n\n\n    \n\n\n\n    \n        as i recall from my bout with kidney stones, there isn't any\nmedication that can do anything about them except relieve the pain.\n\neither they pass, or they have to be broken up with sound, or they have\nto be extracted surgically.\n\nwhen i was in, the x-ray tech happened to mention that she'd had kidney\nstones and children, and the childbirth hurt less.\n    \n\n    \n        \n    \n        \n        \n    \n        \n            \n    \n        y=soc.religion.christian\n    \n\n\n    \n    (probability 0.140, score -0.251)\n\ntop features\n        \n    \n    \n\n\n\nContribution?\nFeature\n\n\n\n\n+0.130\nx7374\n\n\n+0.080\nx27812\n\n\n+0.052\nx21986\n\n\n+0.046\n&lt;BIAS&gt;\n\n\n+0.027\nx15268\n\n\n+0.024\nx6321\n\n\n+0.016\nx14454\n\n\n+0.007\nx12351\n\n\n-0.000\nx24753\n\n\n-0.000\nx7370\n\n\n-0.005\nx28327\n\n\n-0.007\nx6166\n\n\n-0.027\nx28727\n\n\n-0.043\nx21768\n\n\n-0.054\nx19260\n\n\n-0.054\nx27230\n\n\n-0.077\nx24409\n\n\n-0.085\nx19156\n\n\n-0.085\nx17334\n\n\n-0.092\nx16630\n\n\n-0.104\nx24272\n\n\n\n\n\n    \n\n\n\n    \n        as i recall from my bout with kidney stones, there isn't any\nmedication that can do anything about them except relieve the pain.\n\neither they pass, or they have to be broken up with sound, or they have\nto be extracted surgically.\n\nwhen i was in, the x-ray tech happened to mention that she'd had kidney\nstones and children, and the childbirth hurt less.\n    \n\n    \n\n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n\n\n\nThe accuracy of the model is now 0.868, which is better than the previous (0.840), but still not as good as the first (0.889).\nBut the highlighted words seems to be more relevant.\nWe can also experiment with using TFIDF vectorizer.\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvec = TfidfVectorizer(stop_words=\"english\")\nclf = LogisticRegressionCV(max_iter=1_000)\npipe = make_pipeline(vec, clf)\npipe.fit(twenty_train.data, twenty_train.target)\n\nprint_report(pipe)\n\n                        precision    recall  f1-score   support\n\n           alt.atheism       0.92      0.80      0.86       319\n         comp.graphics       0.90      0.96      0.93       389\n               sci.med       0.95      0.92      0.93       396\nsoc.religion.christian       0.89      0.94      0.91       398\n\n              accuracy                           0.91      1502\n             macro avg       0.91      0.91      0.91      1502\n          weighted avg       0.91      0.91      0.91      1502\n\naccuracy: 0.911\n\n\n\neli5.show_prediction(\n    clf,\n    twenty_test.data[0],\n    vec=vec,\n    target_names=twenty_test.target_names,\n    targets=[\"sci.med\"],\n)\n\n\n    \n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n        \n\n    \n\n        \n\n        \n    \n        \n        \n    \n        \n            \n    \n        y=sci.med\n    \n\n\n    \n    (probability 0.827, score 2.048)\n\ntop features\n        \n    \n    \n\n\n\nContribution?\nFeature\n\n\n\n\n+0.510\nx17334\n\n\n+0.447\nx21768\n\n\n+0.330\nx19156\n\n\n+0.245\n&lt;BIAS&gt;\n\n\n+0.193\nx24409\n\n\n+0.186\nx27812\n\n\n+0.150\nx27230\n\n\n+0.109\nx28727\n\n\n+0.053\nx15268\n\n\n+0.052\nx28327\n\n\n+0.038\nx12351\n\n\n+0.029\nx7370\n\n\n+0.024\nx24753\n\n\n-0.002\nx6166\n\n\n-0.003\nx14454\n\n\n-0.022\nx6321\n\n\n-0.032\nx19260\n\n\n-0.032\nx7374\n\n\n-0.049\nx16630\n\n\n-0.052\nx21986\n\n\n-0.126\nx24272\n\n\n\n\n\n    \n\n\n\n    \n        as i recall from my bout with kidney stones, there isn't any\nmedication that can do anything about them except relieve the pain.\n\neither they pass, or they have to be broken up with sound, or they have\nto be extracted surgically.\n\nwhen i was in, the x-ray tech happened to mention that she'd had kidney\nstones and children, and the childbirth hurt less.",
    "crumbs": [
      "Pipeline Improvement"
    ]
  },
  {
    "objectID": "01_debugging_scikit_learn_text_classification_pipeline.html#debugging-hashingvectorizer",
    "href": "01_debugging_scikit_learn_text_classification_pipeline.html#debugging-hashingvectorizer",
    "title": "python applied machine learning",
    "section": "2 Debugging HashingVectorizer",
    "text": "2 Debugging HashingVectorizer\nInstead of char n-grams, we try fitting word n-grams.\nTo handle large vocabularies, we can use HashingVectorizer, to make training faster we can employ SGDClassifier:\n\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.linear_model import SGDClassifier\n\nvec = HashingVectorizer(stop_words=\"english\", ngram_range=(1, 2))\nclf = SGDClassifier(max_iter=20, random_state=42)\npipe = make_pipeline(vec, clf)\npipe.fit(twenty_train.data, twenty_train.target)\n\nprint_report(pipe)\n\n                        precision    recall  f1-score   support\n\n           alt.atheism       0.92      0.79      0.85       319\n         comp.graphics       0.87      0.96      0.92       389\n               sci.med       0.93      0.89      0.91       396\nsoc.religion.christian       0.88      0.93      0.91       398\n\n              accuracy                           0.90      1502\n             macro avg       0.90      0.89      0.90      1502\n          weighted avg       0.90      0.90      0.90      1502\n\naccuracy: 0.899\n\n\nThe classifier ran super fast. Let’s check what the model learned.\n\neli5.show_prediction(\n    clf,\n    twenty_test.data[0],\n    vec=vec,\n    target_names=twenty_test.target_names,\n    targets=[\"sci.med\"],\n)\n\n\n    \n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n        \n\n    \n\n        \n\n        \n    \n        \n        \n    \n        \n            \n    \n        y=sci.med\n    \n\n\n    \n    (score 0.225)\n\ntop features\n        \n    \n    \n\n\n\nContribution?\nFeature\n\n\n\n\n+0.783\nHighlighted in text (sum)\n\n\n-0.558\n&lt;BIAS&gt;\n\n\n\n\n\n    \n\n\n\n    \n        as i recall from my bout with kidney stones, there isn't any\nmedication that can do anything about them except relieve the pain.\n\neither they pass, or they have to be broken up with sound, or they have\nto be extracted surgically.\n\nwhen i was in, the x-ray tech happened to mention that she'd had kidney\nstones and children, and the childbirth hurt less.\n    \n\n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n\n\n\nThe result looks similar to CountVectorizer. But with HashingVectorizer we don’t even have a vocabulary. How does this work?\n\neli5.show_weights(clf, vec=vec, top=10, target_names=twenty_test.target_names)\n\n\n    \n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n        \n\n    \n        \n\n\n\n\n\n\n\n\n\ny=alt.atheism top features\ny=comp.graphics top features\ny=sci.med top features\ny=soc.religion.christian top features\n\n\n\n\n\nWeight?\nFeature\n\n\n\n\n+2.709\nx199378\n\n\n+2.601\nx938889\n\n\n+1.820\nx349126\n\n\n+1.691\nx718537\n\n\n+1.562\nx242643\n\n\n+1.522\nx71928\n\n\n… 53649 more positive …\n\n\n… 53981 more negative …\n\n\n-1.694\nx683213\n\n\n-1.757\nx741207\n\n\n-1.874\nx199709\n\n\n-1.982\nx641063\n\n\n\n\n\n\nWeight?\nFeature\n\n\n\n\n+3.530\nx580586\n\n\n+1.848\nx342790\n\n\n+1.747\nx771885\n\n\n+1.716\nx363686\n\n\n+1.663\nx111283\n\n\n… 32802 more positive …\n\n\n… 32516 more negative …\n\n\n-1.643\nx1031983\n\n\n-1.669\nx85557\n\n\n-1.851\nx120354\n\n\n-1.991\nx693269\n\n\n-2.257\nx814572\n\n\n\n\n\n\nWeight?\nFeature\n\n\n\n\n+2.206\nx988761\n\n\n+2.135\nx337555\n\n\n+1.937\nx154565\n\n\n+1.683\nx806262\n\n\n… 46295 more positive …\n\n\n… 46148 more negative …\n\n\n-1.661\nx34701\n\n\n-1.721\nx354651\n\n\n-1.734\nx790864\n\n\n-1.956\nx85557\n\n\n-2.006\nx365313\n\n\n-2.081\nx494508\n\n\n\n\n\n\nWeight?\nFeature\n\n\n\n\n+3.150\nx641063\n\n\n+2.955\nx199709\n\n\n+2.793\nx741207\n\n\n+2.034\nx396081\n\n\n+1.778\nx274863\n\n\n… 55186 more positive …\n\n\n… 55313 more negative …\n\n\n-1.983\nx672777\n\n\n-2.066\nx443433\n\n\n-2.087\nx199378\n\n\n-2.756\nx718537\n\n\n-3.136\nx970058\n\n\n\n\n\n\n\n    \n\n    \n        \n\n\n    \n        \n\n\n    \n        \n\n\n    \n        \n\n\n    \n\n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n\n\n\nThe feature is not what we expect - since we don’t have vocabulary, we don’t have feature names. We can use InvertableHashingVectorizer to get feature names for HashingVectorizer without fitting a huge vocabulary.\nWe still need some data to learn the words to hash mapping, and we can use a random subset of data to fit it.\n\nimport numpy as np\nfrom eli5.sklearn import InvertableHashingVectorizer\n\n\nivec = InvertableHashingVectorizer(vec)\nsample_size = len(twenty_train.data) // 10\nX_sample = np.random.choice(twenty_train.data, size=sample_size)\nivec.fit(X_sample)\n\nInvertableHashingVectorizer(vec=HashingVectorizer(ngram_range=(1, 2),\n                                                  stop_words='english'))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.InvertableHashingVectorizerInvertableHashingVectorizer(vec=HashingVectorizer(ngram_range=(1, 2),\n                                                  stop_words='english'))vec: HashingVectorizerHashingVectorizer(ngram_range=(1, 2), stop_words='english')HashingVectorizerHashingVectorizer(ngram_range=(1, 2), stop_words='english')\n\n\n\neli5.show_weights(clf, vec=ivec, top=20, target_names=twenty_test.target_names)\n\n\n    \n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n        \n\n    \n        \n\n\n\n\n\n\n\n\n\ny=alt.atheism top features\ny=comp.graphics top features\ny=sci.med top features\ny=soc.religion.christian top features\n\n\n\n\n\nWeight?\nFeature\n\n\n\n\n+2.709\natheism\n\n\n+2.601\nwrites\n\n\n+1.820\nmotto\n\n\n+1.694\nmorality\n\n\n+1.562\nreligion\n\n\n+1.522\nislam\n\n\n+1.502\nwrote\n\n\n+1.451\nkeith\n\n\n+1.392\nobjective\n\n\n+1.357\nlivesey\n\n\n+1.354\nfaq\n\n\n+1.326\nreligious\n\n\n+1.268\natheist\n\n\n+1.258\natheists\n\n\n+1.243\nislamic\n\n\n+1.241\npunishment\n\n\n… 50328 more positive …\n\n\n… 57292 more negative …\n\n\n-1.691\nrutgers edu\n\n\n-1.757\nrutgers\n\n\n-1.874\nchrist\n\n\n-1.982\nchristians\n\n\n\n\n\n\nWeight?\nFeature\n\n\n\n\n+3.530\ngraphics\n\n\n+2.257\nimage\n\n\n+1.991\n3d\n\n\n+1.851\nfiles\n\n\n+1.848\ncode\n\n\n+1.747\nimages\n\n\n+1.716\nsoftware\n\n\n+1.663\nfile\n\n\n+1.643\ncard\n\n\n+1.600\ncomputer\n\n\n+1.580\nvideo\n\n\n+1.527\nFEATURE[335136]\n\n\n+1.527\nftp\n\n\n+1.522\nlooking\n\n\n+1.511\npoints\n\n\n+1.434\nhi\n\n\n+1.410\nalgorithm\n\n\n… 30774 more positive …\n\n\n… 34534 more negative …\n\n\n-1.495\npeople\n\n\n-1.569\nkeyboard\n\n\n-1.669\ngod\n\n\n\n\n\n\nWeight?\nFeature\n\n\n\n\n+2.206\nhealth\n\n\n+2.135\nmsg …\n\n\n+2.081\ndisease\n\n\n+2.006\ntreatment\n\n\n+1.937\ndoctor\n\n\n+1.734\npain\n\n\n+1.721\nmelittin\n\n\n+1.683\ncom\n\n\n+1.661\nmedical\n\n\n+1.474\neffects\n\n\n+1.459\ncancer …\n\n\n+1.452\nwater\n\n\n+1.383\ndiet\n\n\n+1.282\nkeyboard\n\n\n… 44925 more positive …\n\n\n… 47508 more negative …\n\n\n-1.294\nchristian\n\n\n-1.307\nbible\n\n\n-1.309\nreligion\n\n\n-1.421\nchurch\n\n\n-1.612\ngraphics\n\n\n-1.956\ngod\n\n\n\n\n\n\nWeight?\nFeature\n\n\n\n\n+3.150\nchristians\n\n\n+3.136\nchurch\n\n\n+2.955\nchrist\n\n\n+2.793\nrutgers\n\n\n+2.756\nrutgers edu\n\n\n+2.066\nchristian\n\n\n+2.034\nheaven\n\n\n+1.983\nlove\n\n\n+1.778\nathos\n\n\n+1.778\nathos rutgers\n\n\n+1.683\nfaith\n\n\n+1.665\nunderstanding\n\n\n+1.646\n1993\n\n\n+1.637\narticle apr\n\n\n+1.620\nauthority\n\n\n+1.544\nsin\n\n\n+1.541\nsatan\n\n\n+1.469\njesus\n\n\n+1.457\ngod\n\n\n… 53675 more positive …\n\n\n… 56814 more negative …\n\n\n-2.087\natheism\n\n\n\n\n\n\n\n    \n\n    \n        \n\n\n    \n        \n\n\n    \n        \n\n\n    \n        \n\n\n    \n\n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n\n\n\nThere are collisions (hover mouse over features with “…”), and there important features which were not seen in the random sample (FEATURE[…]), but overall it looks fine.\nThe rutgets edu bigram is suspicious though, it looks like a part of email.\n\nrutgers_example = [x for x in twenty_train.data if \"rutgers\" in x.lower()][0]\nrutgers_example\n\n\"In article &lt;Apr.8.00.57.41.1993.28246@athos.rutgers.edu&gt; REXLEX@fnal.gov writes:\\n&gt;In article &lt;Apr.7.01.56.56.1993.22824@athos.rutgers.edu&gt; shrum@hpfcso.fc.hp.com\\n&gt;Matt. 22:9-14 'Go therefore to the main highways, and as many as you find\\n&gt;there, invite to the wedding feast.'...\\n\\n&gt;hmmmmmm.  Sounds like your theology and Christ's are at odds. Which one am I \\n&gt;to believe?\"\n\n\nYup, it seems like the model learned this email address instead of something useful.\n\neli5.show_prediction(\n    clf,\n    rutgers_example,\n    vec=vec,\n    target_names=twenty_test.target_names,\n    targets=[\"soc.religion.christian\"],\n)\n\n\n    \n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n        \n\n    \n\n        \n\n        \n    \n        \n        \n    \n        \n            \n    \n        y=soc.religion.christian\n    \n\n\n    \n    (score 1.978)\n\ntop features\n        \n    \n    \n\n\n\nContribution?\nFeature\n\n\n\n\n+2.645\nHighlighted in text (sum)\n\n\n-0.667\n&lt;BIAS&gt;\n\n\n\n\n\n    \n\n\n\n    \n        in article &lt;apr.8.00.57.41.1993.28246@athos.rutgers.edu&gt; rexlex@fnal.gov writes:\n&gt;in article &lt;apr.7.01.56.56.1993.22824@athos.rutgers.edu&gt; shrum@hpfcso.fc.hp.com\n&gt;matt. 22:9-14 'go therefore to the main highways, and as many as you find\n&gt;there, invite to the wedding feast.'...\n\n&gt;hmmmmmm.  sounds like your theology and christ's are at odds. which one am i \n&gt;to believe?",
    "crumbs": [
      "Pipeline Improvement"
    ]
  },
  {
    "objectID": "refresher/02_debugging_black_box_text_classifier.html",
    "href": "refresher/02_debugging_black_box_text_classifier.html",
    "title": "TextExplainer: Debugging black box text classifier",
    "section": "",
    "text": "eli5.lime can help when it is hard to get exact mapping between model coefficients and text features, e.g. if there are dimensionality reduction involved 1.",
    "crumbs": [
      "Refresher",
      "TextExplainer: Debugging black box text classifier"
    ]
  },
  {
    "objectID": "refresher/02_debugging_black_box_text_classifier.html#example-problem-lsasvm-for-20-newsgroups-dataset",
    "href": "refresher/02_debugging_black_box_text_classifier.html#example-problem-lsasvm-for-20-newsgroups-dataset",
    "title": "TextExplainer: Debugging black box text classifier",
    "section": "1 Example problem: LSA+SVM for 20 Newsgroups dataset",
    "text": "1 Example problem: LSA+SVM for 20 Newsgroups dataset\nWe will create a sample text processing pipeline which is hard to debug using conventional method: SVM with RBF kernel trained on LSA features.\n\nfrom sklearn.datasets import fetch_20newsgroups\n\ncategories = [\"alt.atheism\", \"soc.religion.christian\", \"comp.graphics\", \"sci.med\"]\n\ntwenty_train = fetch_20newsgroups(\n    subset=\"train\",\n    categories=categories,\n    shuffle=True,\n    random_state=42,\n    remove=[\"headers\", \"footers\"],\n)\ntwenty_test = fetch_20newsgroups(\n    subset=\"test\",\n    categories=categories,\n    shuffle=True,\n    random_state=42,\n    remove=[\"headers\", \"footers\"],\n)\n\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.svm import SVC\n\nvec = TfidfVectorizer(min_df=3, stop_words=\"english\", ngram_range=(1, 2))\nsvd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)\nlsa = make_pipeline(vec, svd)\n\nclf = SVC(C=150, gamma=2e-2, probability=True)\npipe = make_pipeline(lsa, clf)\npipe.fit(twenty_train.data, twenty_train.target)\npipe.score(twenty_test.data, twenty_test.target)\n\n0.8901464713715047\n\n\n\ndef print_prediction(doc):\n    y_pred = pipe.predict_proba([doc])[0]\n    for target, prob in zip(twenty_train.target_names, y_pred):\n        print(\"{:.3f} {}\".format(prob, target))\n\n\ndoc = twenty_test.data[0]\nprint_prediction(doc)\n\n0.001 alt.atheism\n0.001 comp.graphics\n0.996 sci.med\n0.003 soc.religion.christian\n\n\nThe classifier predicts that the first document belongs to sci.med with high probability.",
    "crumbs": [
      "Refresher",
      "TextExplainer: Debugging black box text classifier"
    ]
  },
  {
    "objectID": "refresher/02_debugging_black_box_text_classifier.html#textexplainer",
    "href": "refresher/02_debugging_black_box_text_classifier.html#textexplainer",
    "title": "TextExplainer: Debugging black box text classifier",
    "section": "2 TextExplainer",
    "text": "2 TextExplainer\neli5 does not support the pipeline directly, but we can use TextExplainer.\nWe create an instance of TextClassifier, then pass the doc and the black-box classifier which implements predict_proba method to the fit method.\n\nimport eli5\nfrom eli5.lime import TextExplainer\n\nte = TextExplainer(random_state=42)\nte.fit(doc, pipe.predict_proba)\nte.show_prediction(target_names=twenty_train.target_names)\n\n/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n  warnings.warn(\n/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n  warnings.warn(msg, category=FutureWarning)\n\n\n\n    \n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n        \n\n    \n\n    \n        \n    \n        \n        \n    \n        \n            \n    \n        y=alt.atheism\n    \n\n\n    \n    (probability 0.000, score -8.501)\n\ntop features\n        \n    \n    \n\n\n\nContribution?\nFeature\n\n\n\n\n-0.407\n&lt;BIAS&gt;\n\n\n-8.094\nHighlighted in text (sum)\n\n\n\n\n\n    \n\n\n\n    \n        as i recall from my bout with kidney stones, there isn't any\nmedication that can do anything about them except relieve the pain.\n\neither they pass, or they have to be broken up with sound, or they have\nto be extracted surgically.\n\nwhen i was in, the x-ray tech happened to mention that she'd had kidney\nstones and children, and the childbirth hurt less.\n    \n\n    \n        \n    \n        \n        \n    \n        \n            \n    \n        y=comp.graphics\n    \n\n\n    \n    (probability 0.000, score -8.932)\n\ntop features\n        \n    \n    \n\n\n\nContribution?\nFeature\n\n\n\n\n-0.275\n&lt;BIAS&gt;\n\n\n-8.657\nHighlighted in text (sum)\n\n\n\n\n\n    \n\n\n\n    \n        as i recall from my bout with kidney stones, there isn't any\nmedication that can do anything about them except relieve the pain.\n\neither they pass, or they have to be broken up with sound, or they have\nto be extracted surgically.\n\nwhen i was in, the x-ray tech happened to mention that she'd had kidney\nstones and children, and the childbirth hurt less.\n    \n\n    \n        \n    \n        \n        \n    \n        \n            \n    \n        y=sci.med\n    \n\n\n    \n    (probability 0.996, score 7.104)\n\ntop features\n        \n    \n    \n\n\n\nContribution?\nFeature\n\n\n\n\n+7.161\nHighlighted in text (sum)\n\n\n-0.057\n&lt;BIAS&gt;\n\n\n\n\n\n    \n\n\n\n    \n        as i recall from my bout with kidney stones, there isn't any\nmedication that can do anything about them except relieve the pain.\n\neither they pass, or they have to be broken up with sound, or they have\nto be extracted surgically.\n\nwhen i was in, the x-ray tech happened to mention that she'd had kidney\nstones and children, and the childbirth hurt less.\n    \n\n    \n        \n    \n        \n        \n    \n        \n            \n    \n        y=soc.religion.christian\n    \n\n\n    \n    (probability 0.003, score -5.711)\n\ntop features\n        \n    \n    \n\n\n\nContribution?\nFeature\n\n\n\n\n-0.322\n&lt;BIAS&gt;\n\n\n-5.389\nHighlighted in text (sum)\n\n\n\n\n\n    \n\n\n\n    \n        as i recall from my bout with kidney stones, there isn't any\nmedication that can do anything about them except relieve the pain.\n\neither they pass, or they have to be broken up with sound, or they have\nto be extracted surgically.\n\nwhen i was in, the x-ray tech happened to mention that she'd had kidney\nstones and children, and the childbirth hurt less.",
    "crumbs": [
      "Refresher",
      "TextExplainer: Debugging black box text classifier"
    ]
  },
  {
    "objectID": "refresher/02_debugging_black_box_text_classifier.html#sanity-check",
    "href": "refresher/02_debugging_black_box_text_classifier.html#sanity-check",
    "title": "TextExplainer: Debugging black box text classifier",
    "section": "3 Sanity check",
    "text": "3 Sanity check\nWe can do a sanity check by removing the highlighted words and check how the prediction changes.\n\nimport re\n\ndoc2 = re.sub(r\"(recall|kidney|stones|medication|pain|tech)\", \"\", doc, flags=re.I)\nprint_prediction(doc2)\n\n0.067 alt.atheism\n0.150 comp.graphics\n0.354 sci.med\n0.428 soc.religion.christian\n\n\nThe predicted probabilities is indeed lower now.\n\nprint(te.samples_[0])\n\nAs    my   kidney ,  isn' any\n  can        .\n\nEither they ,     be    ,   \nto   .\n\n   ,  - tech  to mention  ' had kidney\n and ,     .\n\n\n\n# By default, TextExplainer generates 5000 distorted texts.\nlen(te.samples_)\n\n5000\n\n\n\nte.vec_, te.clf_\n\n(CountVectorizer(ngram_range=(1, 2), token_pattern='(?u)\\\\b\\\\w+\\\\b'),\n SGDClassifier(alpha=0.001, loss='log', penalty='elasticnet',\n               random_state=RandomState(MT19937) at 0x13144B040))\n\n\n\nte.metrics_\n\n{'mean_KL_divergence': 0.019433059538127587, 'score': 0.9850240438373423}",
    "crumbs": [
      "Refresher",
      "TextExplainer: Debugging black box text classifier"
    ]
  },
  {
    "objectID": "refresher/02_debugging_black_box_text_classifier.html#customizing-textexplainer-classifier",
    "href": "refresher/02_debugging_black_box_text_classifier.html#customizing-textexplainer-classifier",
    "title": "TextExplainer: Debugging black box text classifier",
    "section": "4 Customizing TextExplainer: classifier",
    "text": "4 Customizing TextExplainer: classifier\nWe can replace the default SGDClassifier (observed with te.clf_) with DecisionTreeClassifier.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nte5 = TextExplainer(clf=DecisionTreeClassifier(max_depth=2), random_state=0)\nte5.fit(doc, pipe.predict_proba)\nprint(te5.metrics_)\nte5.show_weights()\n\n{'mean_KL_divergence': 0.03942534257087394, 'score': 0.9820333011866811}\n\n\n/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n  warnings.warn(msg, category=FutureWarning)\n\n\n\n    \n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n        \n\n\n\nWeight\nFeature\n\n\n\n\n0.5441\nkidney\n\n\n0.4559\npain\n\n\n0\nless\n\n\n0\nextracted\n\n\n0\nextracted surgically\n\n\n0\nfrom\n\n\n0\nfrom my\n\n\n0\nhad\n\n\n0\nhad kidney\n\n\n0\nhappened\n\n\n0\nhappened to\n\n\n0\nhave\n\n\n0\nhave to\n\n\n0\nmedication\n\n\n0\nhurt less\n\n\n0\nisn t\n\n\n0\nhurt\n\n\n0\nkidney stones\n\n\n0\nexcept relieve\n\n\n0\nx ray\n\n\n… 93 more …\n\n\n\n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n        \n        \n        \n\nTree\n\n\n\n0\n\nkidney &lt;= 0.5\ngini = 0.158\nsamples = 100.0%\nvalue = [0.01, 0.033, 0.916, 0.041]\n\n\n\n1\n\npain &lt;= 0.5\ngini = 0.389\nsamples = 38.9%\nvalue = [0.028, 0.096, 0.768, 0.109]\n\n\n\n0-&gt;1\n\n\nTrue\n\n\n\n4\n\npain &lt;= 0.5\ngini = 0.045\nsamples = 61.1%\nvalue = [0.003, 0.007, 0.977, 0.013]\n\n\n\n0-&gt;4\n\n\nFalse\n\n\n\n2\n\ngini = 0.526\nsamples = 28.4%\nvalue = [0.042, 0.146, 0.652, 0.16]\n\n\n\n1-&gt;2\n\n\n\n\n\n3\n\ngini = 0.041\nsamples = 10.6%\nvalue = [0.002, 0.005, 0.979, 0.014]\n\n\n\n1-&gt;3\n\n\n\n\n\n5\n\ngini = 0.116\nsamples = 22.8%\nvalue = [0.007, 0.02, 0.939, 0.033]\n\n\n\n4-&gt;5\n\n\n\n\n\n6\n\ngini = 0.01\nsamples = 38.2%\nvalue = [0.001, 0.001, 0.995, 0.003]\n\n\n\n4-&gt;6\n\n\n\n\n\n\n    \n\n\n\n\n\nIf kidney &lt;= 0.5, it means the word kidney is not in the document.\nSo according to this tree, if the word kidney and pain is not in the document, the probability that this document belongs to sci.med drops to 0.65 (the bottom left node value, the third item in the value array). If at least one of these words remain, sci.med probability is 0.9+.\n\nprint(\"both words removed:\")\nprint_prediction(re.sub(r\"(kidney|pain)\", \"\", doc, flags=re.I))\nprint()\nprint('only \"pain\" removed:')\nprint_prediction(re.sub(r\"(pain)\", \"\", doc, flags=re.I))\n\nboth words removed:\n0.014 alt.atheism\n0.023 comp.graphics\n0.895 sci.med\n0.068 soc.religion.christian\n\nonly \"pain\" removed:\n0.003 alt.atheism\n0.004 comp.graphics\n0.981 sci.med\n0.013 soc.religion.christian",
    "crumbs": [
      "Refresher",
      "TextExplainer: Debugging black box text classifier"
    ]
  },
  {
    "objectID": "refresher/02_debugging_black_box_text_classifier.html#footnotes",
    "href": "refresher/02_debugging_black_box_text_classifier.html#footnotes",
    "title": "TextExplainer: Debugging black box text classifier",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://eli5.readthedocs.io/en/latest/tutorials/black-box-text-classifiers.html↩︎",
    "crumbs": [
      "Refresher",
      "TextExplainer: Debugging black box text classifier"
    ]
  }
]